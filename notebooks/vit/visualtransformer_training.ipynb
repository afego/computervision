{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\GitHub\\computervision\n"
     ]
    }
   ],
   "source": [
    "cd E:/GitHub/computervision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "# os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from datetime import datetime\n",
    "from torch.optim import lr_scheduler, SGD, Adamax\n",
    "from modules.training_util import PytorchDataset, PytorchTraining, EarlyStopping\n",
    "from modules.models_util import ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for continuing previous training\n",
    "CONTINUE_TRAINING_DIR = None\n",
    "# CONTINUE_TRAINING_DIR = 'notebooks/vit/models/'\n",
    "\n",
    "MODEL_WEIGHTS_PATH = None\n",
    "\n",
    "if CONTINUE_TRAINING_DIR:\n",
    "    MODEL_WEIGHTS_PATH = os.path.join(CONTINUE_TRAINING_DIR,'epoch_15.pth')\n",
    "\n",
    "# CONFIG VARIABLES\n",
    "FULLY_TRAIN = True\n",
    "base_path = 'E:/Datasets/cor-splits'\n",
    "DATA_DIR = [\n",
    "    # os.path.join(base_path,'sgkf-8-1-1-size-540'),\n",
    "    # os.path.join(base_path,'balanced_train'),\n",
    "    # os.path.join(base_path,'imbalanced_train'),\n",
    "    # os.path.join(base_path,'full_imbalanced_train'),\n",
    "    # os.path.join(base_path,'natural_imbalanced_train'),\n",
    "    os.path.join(base_path,'underperforming_cameras_96'),\n",
    "    # os.path.join(base_path,'natural'),\n",
    "    # os.path.join(base_path,'balanced_positive'),\n",
    "    # os.path.join(base_path,'natural_positive'),\n",
    "    # os.path.join(base_path,'balanced')\n",
    "    ]\n",
    "START_EPOCH = 1 # Change it from 1 if you're continuing an existing run\n",
    "N_EPOCHS = 100\n",
    "BATCH_SIZE = 16\n",
    "SAVE_INTERVAL = 4 # How many epochs between each backup\n",
    "## Learning Rate Scheduler\n",
    "LR = [0.001]  # When continuing a existing run, update it to the last epoch LR value\n",
    "LR_EPOCH_SCHEDULE = 10  # Number of epochs before altering LR\n",
    "LR_GAMMA = 0.1\n",
    "## Early Stopping\n",
    "PATIENCE = LR_EPOCH_SCHEDULE + 1\n",
    "MIN_DELTA = 0\n",
    "MIN_EPOCH = 10 # Number of spochs before starting patience counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device = cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device = {device}\")\n",
    "archs = ['b32']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All parameters for model VisionTransformer requires grad.\n",
      "Saving model to folder notebooks/vit\\models\\2024-03-28 07-16-58\n",
      "\n",
      "Model VisionTransformer b32\n",
      "Fully Trained = True\n",
      "Dataset E:/Datasets/cor-splits\\underperforming_cameras_96\n",
      "Learning Rate Epoch Schedule = 10\n",
      "Learning Rate Gamma = 0.1\n",
      "Optimizer = SGD\n",
      "Batch Size = 16\n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "----------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Adm\\anaconda3\\envs\\computervision\\Lib\\site-packages\\torch\\nn\\functional.py:5476: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.5997 Acc: 0.6798\n",
      "val Loss: 0.5112 Acc: 0.7013\n",
      "Epoch duration: 2 m 3s\n",
      "Learning Rate = 0.001\n",
      "\n",
      "\n",
      "\n",
      "Epoch 2/100\n",
      "----------\n",
      "\n",
      "train Loss: 0.4125 Acc: 0.8319\n",
      "val Loss: 0.7938 Acc: 0.5551\n",
      "Epoch duration: 1 m 9s\n",
      "Learning Rate = 0.001\n",
      "\n",
      "\n",
      "\n",
      "Epoch 3/100\n",
      "----------\n",
      "\n",
      "train Loss: 0.2800 Acc: 0.8982\n",
      "val Loss: 0.7209 Acc: 0.6115\n",
      "Epoch duration: 1 m 8s\n",
      "Learning Rate = 0.001\n",
      "\n",
      "\n",
      "\n",
      "Epoch 4/100\n",
      "----------\n",
      "\n",
      "train Loss: 0.1952 Acc: 0.9323\n",
      "Model saved at notebooks/vit\\models\\2024-03-28 07-16-58/epoch_4.pth\n",
      "val Loss: 0.4066 Acc: 0.8154\n",
      "Model saved at notebooks/vit\\models\\2024-03-28 07-16-58/epoch_4.pth\n",
      "Epoch duration: 1 m 9s\n",
      "Learning Rate = 0.001\n",
      "\n",
      "\n",
      "\n",
      "Epoch 5/100\n",
      "----------\n",
      "\n",
      "train Loss: 0.1355 Acc: 0.9537\n",
      "val Loss: 0.5244 Acc: 0.7372\n",
      "Epoch duration: 1 m 8s\n",
      "Learning Rate = 0.001\n",
      "\n",
      "\n",
      "\n",
      "Epoch 6/100\n",
      "----------\n",
      "\n",
      "train Loss: 0.0909 Acc: 0.9720\n",
      "val Loss: 0.3451 Acc: 0.8449\n",
      "Epoch duration: 1 m 8s\n",
      "Learning Rate = 0.001\n",
      "\n",
      "\n",
      "\n",
      "Epoch 7/100\n",
      "----------\n",
      "\n",
      "train Loss: 0.0655 Acc: 0.9791\n",
      "val Loss: 0.4078 Acc: 0.8295\n",
      "Epoch duration: 1 m 8s\n",
      "Learning Rate = 0.001\n",
      "\n",
      "\n",
      "\n",
      "Epoch 8/100\n",
      "----------\n",
      "\n",
      "train Loss: 0.0434 Acc: 0.9882\n",
      "Model saved at notebooks/vit\\models\\2024-03-28 07-16-58/epoch_8.pth\n",
      "val Loss: 1.2418 Acc: 0.6013\n",
      "Model saved at notebooks/vit\\models\\2024-03-28 07-16-58/epoch_8.pth\n",
      "Epoch duration: 1 m 9s\n",
      "Learning Rate = 0.001\n",
      "\n",
      "\n",
      "\n",
      "Epoch 9/100\n",
      "----------\n",
      "\n",
      "train Loss: 0.0400 Acc: 0.9887\n",
      "val Loss: 0.6818 Acc: 0.7423\n",
      "Epoch duration: 1 m 2s\n",
      "Learning Rate = 0.001\n",
      "\n",
      "\n",
      "\n",
      "Epoch 10/100\n",
      "----------\n",
      "\n",
      "train Loss: 0.0266 Acc: 0.9918\n",
      "val Loss: 0.3856 Acc: 0.8423\n",
      "Epoch duration: 1 m 5s\n",
      "Learning Rate = 0.0001\n",
      "\n",
      "\n",
      "\n",
      "Epoch 11/100\n",
      "----------\n",
      "\n",
      "train Loss: 0.0134 Acc: 0.9969\n",
      "val Loss: 0.3690 Acc: 0.8526\n",
      "Epoch duration: 1 m 6s\n",
      "Learning Rate = 0.0001\n",
      "Lower loss found, resetting patience counter\n",
      "\n",
      "\n",
      "Epoch 12/100\n",
      "----------\n",
      "\n",
      "train Loss: 0.0095 Acc: 0.9988\n",
      "Model saved at notebooks/vit\\models\\2024-03-28 07-16-58/epoch_12.pth\n",
      "val Loss: 0.3737 Acc: 0.8526\n",
      "Model saved at notebooks/vit\\models\\2024-03-28 07-16-58/epoch_12.pth\n",
      "Epoch duration: 1 m 8s\n",
      "Learning Rate = 0.0001\n",
      "Loss didnt decrease from 0.3690. Increasing patience counter to 1\n",
      "\n",
      "\n",
      "Epoch 13/100\n",
      "----------\n",
      "\n",
      "train Loss: 0.0089 Acc: 0.9990\n",
      "val Loss: 0.3643 Acc: 0.8590\n",
      "Epoch duration: 1 m 4s\n",
      "Learning Rate = 0.0001\n",
      "Lower loss found, resetting patience counter\n",
      "\n",
      "\n",
      "Epoch 14/100\n",
      "----------\n",
      "\n",
      "train Loss: 0.0085 Acc: 0.9988\n",
      "val Loss: 0.3668 Acc: 0.8577\n",
      "Epoch duration: 1 m 4s\n",
      "Learning Rate = 0.0001\n",
      "Loss didnt decrease from 0.3643. Increasing patience counter to 1\n",
      "\n",
      "\n",
      "Epoch 15/100\n",
      "----------\n",
      "\n",
      "train Loss: 0.0074 Acc: 0.9990\n",
      "val Loss: 0.3647 Acc: 0.8551\n",
      "Epoch duration: 1 m 5s\n",
      "Learning Rate = 0.0001\n",
      "Loss didnt decrease from 0.3643. Increasing patience counter to 2\n",
      "\n",
      "\n",
      "Epoch 16/100\n",
      "----------\n",
      "\n",
      "train Loss: 0.0072 Acc: 0.9990\n",
      "Model saved at notebooks/vit\\models\\2024-03-28 07-16-58/epoch_16.pth\n",
      "val Loss: 0.3717 Acc: 0.8577\n",
      "Model saved at notebooks/vit\\models\\2024-03-28 07-16-58/epoch_16.pth\n",
      "Epoch duration: 1 m 5s\n",
      "Learning Rate = 0.0001\n",
      "Loss didnt decrease from 0.3643. Increasing patience counter to 3\n",
      "\n",
      "\n",
      "Epoch 17/100\n",
      "----------\n",
      "\n",
      "train Loss: 0.0067 Acc: 0.9990\n",
      "val Loss: 0.3666 Acc: 0.8564\n",
      "Epoch duration: 1 m 2s\n",
      "Learning Rate = 0.0001\n",
      "Loss didnt decrease from 0.3643. Increasing patience counter to 4\n",
      "\n",
      "\n",
      "Epoch 18/100\n",
      "----------\n",
      "\n",
      "train Loss: 0.0066 Acc: 0.9991\n",
      "val Loss: 0.3704 Acc: 0.8577\n",
      "Epoch duration: 1 m 4s\n",
      "Learning Rate = 0.0001\n",
      "Loss didnt decrease from 0.3643. Increasing patience counter to 5\n",
      "\n",
      "\n",
      "Epoch 19/100\n",
      "----------\n",
      "\n",
      "train Loss: 0.0061 Acc: 0.9993\n",
      "val Loss: 0.3690 Acc: 0.8603\n",
      "Epoch duration: 1 m 12s\n",
      "Learning Rate = 0.0001\n",
      "Loss didnt decrease from 0.3643. Increasing patience counter to 6\n",
      "\n",
      "\n",
      "Epoch 20/100\n",
      "----------\n",
      "\n",
      "train Loss: 0.0062 Acc: 0.9988\n",
      "Model saved at notebooks/vit\\models\\2024-03-28 07-16-58/epoch_20.pth\n",
      "val Loss: 0.3649 Acc: 0.8628\n",
      "Model saved at notebooks/vit\\models\\2024-03-28 07-16-58/epoch_20.pth\n",
      "Epoch duration: 1 m 17s\n",
      "Learning Rate = 1e-05\n",
      "Loss didnt decrease from 0.3643. Increasing patience counter to 7\n",
      "\n",
      "\n",
      "Epoch 21/100\n",
      "----------\n",
      "\n",
      "train Loss: 0.0059 Acc: 0.9991\n",
      "val Loss: 0.3659 Acc: 0.8641\n",
      "Epoch duration: 1 m 0s\n",
      "Learning Rate = 1e-05\n",
      "Loss didnt decrease from 0.3643. Increasing patience counter to 8\n",
      "\n",
      "\n",
      "Epoch 22/100\n",
      "----------\n",
      "\n",
      "train Loss: 0.0057 Acc: 0.9993\n",
      "val Loss: 0.3669 Acc: 0.8641\n",
      "Epoch duration: 0 m 59s\n",
      "Learning Rate = 1e-05\n",
      "Loss didnt decrease from 0.3643. Increasing patience counter to 9\n",
      "\n",
      "\n",
      "Epoch 23/100\n",
      "----------\n",
      "\n",
      "train Loss: 0.0056 Acc: 0.9993\n",
      "val Loss: 0.3671 Acc: 0.8641\n",
      "Epoch duration: 0 m 59s\n",
      "Learning Rate = 1e-05\n",
      "Loss didnt decrease from 0.3643. Increasing patience counter to 10\n",
      "\n",
      "\n",
      "Epoch 24/100\n",
      "----------\n",
      "\n",
      "train Loss: 0.0053 Acc: 0.9997\n",
      "Model saved at notebooks/vit\\models\\2024-03-28 07-16-58/epoch_24.pth\n",
      "val Loss: 0.3676 Acc: 0.8641\n",
      "Model saved at notebooks/vit\\models\\2024-03-28 07-16-58/epoch_24.pth\n",
      "Epoch duration: 1 m 14s\n",
      "Learning Rate = 1e-05\n",
      "Early stopping after 11 epochs\n",
      "\n",
      "Training complete in 27m 32s\n",
      "Best val Acc: 0.864103\n",
      "\n",
      "Confusion Matrix\n",
      "[[324  66]\n",
      " [ 40 350]]\n",
      "\n",
      "Accuracy by class\n",
      "[83.07692308 89.74358974]\n"
     ]
    }
   ],
   "source": [
    "for arch in archs:\n",
    "    for dir in DATA_DIR:\n",
    "        for lr in LR:\n",
    "            vit = ViT(architecture=arch)\n",
    "            \n",
    "            model = vit.load(MODEL_WEIGHTS_PATH, FULLY_TRAIN)\n",
    "            model = model.to(device)\n",
    "            \n",
    "            criterion = CrossEntropyLoss()\n",
    "\n",
    "            if FULLY_TRAIN:\n",
    "                params = model.parameters()\n",
    "            else:\n",
    "                params = model.heads.parameters()\n",
    "                \n",
    "            # optimizer = optim.SGD(model.classifier.parameters(), lr=LR)\n",
    "            optimizer = SGD(params, lr=lr)\n",
    "            # optimizer = Adamax(model.parameters(), lr=LR)\n",
    "\n",
    "            step_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=LR_EPOCH_SCHEDULE, gamma=LR_GAMMA)\n",
    "            es = EarlyStopping(patience=PATIENCE, delta=MIN_DELTA, min_epoch=MIN_EPOCH)\n",
    "            dataset = PytorchDataset(dir, vit.data_transforms, BATCH_SIZE)\n",
    "\n",
    "            working_folder = 'notebooks/vit'\n",
    "            models_folder = 'models'\n",
    "\n",
    "            curr_time = datetime.now()\n",
    "            curr_time = curr_time.strftime(\"%Y-%m-%d %H-%M-%S\")\n",
    "\n",
    "            output_folder = os.path.join(working_folder, models_folder)\n",
    "            output_folder = os.path.join(output_folder, curr_time)\n",
    "            \n",
    "            if not os.path.exists(output_folder):\n",
    "                os.makedirs(output_folder)\n",
    "\n",
    "            trainer = PytorchTraining(device, dataset, output_folder)\n",
    "            model = trainer.train_pytorch_model(model, criterion, optimizer, step_lr_scheduler, es, START_EPOCH, N_EPOCHS, SAVE_INTERVAL, arch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
