{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Aggregated Residual Transformations for Deep Neural Networks\n",
    "\n",
    " Xie, S., Girshick, R., Doll√°r, P., Tu, Z. and He, K., 2017. Aggregated residual transformations for deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 1492-1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:\\GitHub\\computervision\n"
     ]
    }
   ],
   "source": [
    "cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Adm/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnext50_32x4d', pretrained=True)\n",
    "# model = torch.hub.load('pytorch/vision:v0.10.0', 'resnext101_32x8d', pretrained=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r'E:/Datasets/cor/test/normal/153/CODE153_2023-05-30_21-38-49.jpg'\n",
    "# filepath = r'/mnt/e/Datasets/cor/test/normal/153/CODE153_2023-05-30_21-38-49.jpg'\n",
    "input_image = Image.open(filepath)\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "input_tensor = preprocess(input_image)\n",
    "input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is available\n"
     ]
    }
   ],
   "source": [
    "# move the input and model to GPU for speed if available\n",
    "if torch.cuda.is_available():\n",
    "    print('cuda is available')\n",
    "    input_batch = input_batch.to('cuda')\n",
    "    model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.2984e+00, -6.6855e-01, -1.8604e+00, -2.8240e+00, -1.2040e+00,\n",
      "         1.3949e-01, -3.3319e+00, -1.7991e+00, -1.8133e+00, -1.4335e+00,\n",
      "        -1.2880e+00, -5.3990e-01, -3.1225e+00, -2.9961e+00, -3.7404e+00,\n",
      "        -3.3879e+00, -2.4940e+00, -5.3142e+00, -1.7284e+00, -2.6742e+00,\n",
      "        -3.2580e+00, -2.9461e+00, -1.3539e+00, -2.5036e-01, -2.8359e+00,\n",
      "        -2.7875e+00, -1.4053e-01,  3.5596e-01, -6.9057e-02, -1.1228e+00,\n",
      "        -1.3965e+00,  1.3184e+00, -3.7975e-01, -3.7084e-01,  4.5963e-01,\n",
      "        -2.6578e+00, -1.1131e+00, -4.1030e+00,  7.0532e-01, -1.0448e+00,\n",
      "        -9.4203e-01, -9.3312e-01, -3.5354e+00, -1.3928e+00, -9.2698e-01,\n",
      "        -2.0861e+00, -2.1894e+00,  8.7755e-01, -2.1977e+00, -1.6447e+00,\n",
      "        -2.3343e+00,  1.0024e+00,  1.3931e-01,  2.8250e-01, -3.2743e+00,\n",
      "        -1.6212e+00, -2.4345e+00, -1.8232e+00,  2.0898e-01, -3.2414e-01,\n",
      "         3.5253e+00, -1.6873e+00, -1.2566e+00, -8.2623e-01, -8.8792e-01,\n",
      "        -2.5725e-01, -3.3751e-02, -4.4656e-01, -9.3074e-01, -2.0719e+00,\n",
      "         1.4064e-01, -8.1068e-01, -4.0842e+00,  1.0381e+00, -2.5832e+00,\n",
      "        -8.9252e-01, -6.1040e-01, -1.9102e+00,  1.2905e+00,  1.4649e+00,\n",
      "        -1.3654e+00, -3.4637e+00, -2.4416e+00, -3.0042e+00, -9.2974e-01,\n",
      "        -3.3168e+00, -1.0919e+00, -4.2864e+00, -3.4028e+00, -2.1364e+00,\n",
      "        -3.4095e+00, -5.2570e+00, -6.2606e-01, -2.8600e+00, -1.6775e+00,\n",
      "        -2.3822e+00, -1.2414e+00, -7.0671e-01, -1.8084e+00, -4.7019e-01,\n",
      "        -3.0769e-01, -1.3467e+00, -9.3710e-01, -2.2884e+00, -1.1368e+00,\n",
      "        -3.2868e+00, -3.2748e+00,  1.3115e-01,  1.9010e+00, -3.2768e+00,\n",
      "        -6.0608e-01,  2.9533e+00, -5.6701e-01,  1.9353e+00,  4.1263e-01,\n",
      "        -1.9297e+00,  1.6072e-01, -5.7780e-01, -3.8010e-01, -6.3589e-01,\n",
      "         1.7599e+00, -1.7117e+00, -3.9161e+00, -2.7678e+00, -2.2953e+00,\n",
      "        -1.1606e+00,  2.9312e+00, -2.9478e+00, -1.7005e-01, -1.2499e+00,\n",
      "        -6.9390e-01, -3.2924e+00, -1.4888e+00, -4.5753e+00, -2.0172e+00,\n",
      "        -1.4390e+00, -2.1219e+00, -1.4704e+00, -4.4956e+00, -2.1137e+00,\n",
      "        -1.1386e+00, -8.0431e-01, -2.2477e+00, -1.3594e+00,  9.4681e-01,\n",
      "        -1.4868e+00, -1.0650e+00,  2.7754e-01,  8.7097e-02, -2.0465e-01,\n",
      "        -7.5401e-01,  1.5377e+00,  9.1266e-01,  2.3586e+00,  1.2141e+00,\n",
      "         2.2812e+00, -1.5355e+00,  1.7457e+00,  1.4193e+00, -4.2047e-01,\n",
      "         3.6246e-01, -1.2094e+00, -1.8894e+00, -4.9649e-01,  9.7874e-01,\n",
      "        -6.3308e-01, -3.9762e-02, -3.7840e-01,  1.5760e+00,  1.5206e+00,\n",
      "        -6.1745e-01,  8.1043e-01,  9.1656e-02, -1.0567e+00, -1.8629e+00,\n",
      "        -1.5118e+00, -2.8513e-01, -8.6195e-01, -1.5106e+00, -1.0607e+00,\n",
      "        -1.9367e+00, -2.2015e+00, -1.3559e+00, -3.2410e-01, -1.7284e+00,\n",
      "         7.1683e-01,  4.7597e-01,  1.1850e+00,  1.1731e+00, -3.0099e-01,\n",
      "        -1.4807e+00, -1.5511e+00,  4.3856e-01,  6.2548e-01, -2.4474e+00,\n",
      "        -6.0161e-02,  9.1266e-01, -1.2540e+00, -3.0194e+00, -4.9899e-01,\n",
      "         1.8218e+00, -4.2056e-01, -6.1228e-01,  1.7265e+00,  1.1919e+00,\n",
      "        -4.1562e+00, -2.1937e+00,  2.7086e-01,  1.1561e+00, -2.3960e-01,\n",
      "        -1.8712e-01, -5.6187e-01, -1.2900e+00, -3.2945e+00, -2.8625e+00,\n",
      "        -2.2178e+00, -2.2838e+00, -2.7216e+00, -2.2261e+00, -7.1980e-01,\n",
      "        -1.8200e+00, -1.8861e+00, -1.7004e+00,  1.6380e+00,  6.1545e-01,\n",
      "        -3.0321e+00, -8.7018e-01, -8.0614e-01, -4.5730e-01,  8.8687e-01,\n",
      "         2.5532e+00,  3.2692e+00,  3.5830e-01, -2.2745e-01, -4.3645e-01,\n",
      "        -8.9744e-01,  1.4930e-01,  4.8672e-02, -2.0663e+00, -2.0789e+00,\n",
      "        -1.4329e+00, -2.3808e+00, -1.6027e+00, -1.6592e+00, -1.1300e+00,\n",
      "         1.1858e+00, -1.2754e+00, -5.0428e-01,  1.0875e+00,  1.0934e+00,\n",
      "         1.6170e+00,  7.4932e-02,  1.9312e-01,  7.0929e-01,  3.6529e-01,\n",
      "        -2.3448e+00,  8.9600e-02,  2.9440e+00,  1.7615e+00,  4.8181e+00,\n",
      "         5.2056e-01,  5.9482e-01, -5.7571e-01,  9.3045e-01,  1.2890e-01,\n",
      "        -1.3373e-02, -3.0301e-01,  6.5352e-03, -5.0578e-01, -3.2587e-01,\n",
      "         8.5685e-01, -2.9575e+00,  1.9584e+00,  2.6367e+00, -1.5014e-02,\n",
      "        -1.4146e+00, -1.8027e+00,  1.4745e+00, -1.3110e-01,  1.8848e+00,\n",
      "        -2.9029e-01, -8.3765e-01, -2.6139e+00,  7.6482e-01, -1.6051e+00,\n",
      "        -3.6300e-01, -2.0590e+00, -2.5576e+00, -1.3695e+00, -2.2891e+00,\n",
      "        -2.8752e+00, -1.8195e+00, -3.0596e+00, -2.3647e-01, -1.8245e+00,\n",
      "        -1.5061e+00, -1.6182e+00, -1.4834e+00, -2.4954e+00,  2.8122e-01,\n",
      "        -1.3479e+00,  8.2961e-01, -1.0140e+00,  1.9171e-01, -5.1421e-01,\n",
      "        -2.5157e+00, -2.1941e+00, -1.3001e+00, -1.0355e+00, -7.3589e-01,\n",
      "         4.6943e+00, -2.0313e+00, -3.9674e-02, -1.4813e-01, -2.0379e+00,\n",
      "        -1.4304e-02, -2.0941e+00, -1.3843e+00,  1.2808e+00,  2.9124e-01,\n",
      "        -1.9745e-01, -2.5604e+00, -1.3087e+00, -2.3208e+00, -7.0344e-01,\n",
      "        -1.2691e+00, -1.4728e+00,  4.8497e-01,  4.9882e-01, -2.5239e+00,\n",
      "         2.1375e-01, -2.4494e-01,  1.0470e-01, -1.4443e+00,  4.8272e-02,\n",
      "         6.7481e-01, -2.3398e+00,  3.4374e-01, -1.8468e+00, -4.3883e-01,\n",
      "        -1.2630e+00, -1.1487e-01, -1.5626e+00, -3.4819e+00, -2.1747e+00,\n",
      "         7.0735e-01, -7.6326e-01,  8.5259e-02, -9.9110e-01,  2.4115e+00,\n",
      "         3.8084e-01, -2.9498e+00, -1.2360e+00, -5.8756e-01,  1.9612e-01,\n",
      "        -1.0531e+00, -1.5703e+00, -2.0184e+00, -1.5474e+00, -1.2387e+00,\n",
      "        -2.8216e+00,  2.1959e+00, -5.5496e-01, -5.9641e-01, -1.9963e+00,\n",
      "        -2.3979e+00, -4.1630e+00, -2.5450e+00, -1.3710e+00, -1.4164e+00,\n",
      "        -1.9997e+00, -9.3459e-01,  6.3128e-01,  2.5439e+00,  6.8790e-01,\n",
      "        -1.5698e+00, -3.2012e-01,  2.9050e-01,  4.3726e-02, -2.2628e+00,\n",
      "         3.1463e+00, -1.1873e+00,  9.6532e-01, -7.8968e-01, -1.3600e+00,\n",
      "        -3.6067e-01, -2.5515e+00, -2.0896e+00, -3.6880e+00, -3.9707e-01,\n",
      "        -9.8552e-01, -3.5841e+00, -2.3770e+00, -2.0634e+00, -1.2307e+00,\n",
      "        -2.0719e+00, -3.6283e+00, -2.6322e+00, -1.7973e+00,  2.0295e+00,\n",
      "        -3.1583e+00,  2.2737e-01, -1.4851e+00,  5.4620e+00,  4.4537e+00,\n",
      "         3.4746e+00, -1.7297e+00,  4.2128e+00,  3.1159e+00,  2.0273e+00,\n",
      "         2.3498e-01, -2.1985e+00,  3.1924e+00,  8.6488e-01, -1.0210e+00,\n",
      "         1.3810e+00,  3.5535e+00,  4.0091e+00,  2.9704e+00,  1.2485e+00,\n",
      "        -1.4879e+00,  7.5609e+00,  4.2123e-02, -1.4121e+00, -1.3833e+00,\n",
      "         7.1234e-01, -7.6250e-01,  1.0150e-02,  3.2539e+00, -1.4217e+00,\n",
      "         1.2707e+00, -1.7442e+00,  1.8824e-01,  7.6324e-01, -2.8269e+00,\n",
      "         8.7416e-01,  4.4086e+00,  4.5652e+00, -1.1754e+00, -1.9061e-01,\n",
      "        -5.5242e-01,  2.8535e-01,  2.2135e+00, -3.3964e+00,  3.1016e+00,\n",
      "         3.2171e-01,  4.4687e+00,  1.8166e+00, -5.3117e-01,  6.7053e-01,\n",
      "         3.9872e-01, -2.1642e-01, -1.5841e+00, -1.3466e+00, -2.2332e+00,\n",
      "        -4.6774e-01,  2.3048e+00, -7.0527e-01, -2.2730e-01,  9.8865e-01,\n",
      "         7.1168e+00, -1.1298e-01,  1.3840e+00,  4.7910e-01,  2.2601e-02,\n",
      "         5.0408e-01,  5.9583e+00, -2.4439e+00,  8.0469e+00, -1.6120e+00,\n",
      "        -1.0073e+00,  1.0719e+00,  1.7504e-01, -2.0142e+00,  6.4736e-01,\n",
      "         5.9470e+00,  3.8320e+00, -1.0881e+00, -7.1546e-01,  3.8246e+00,\n",
      "         2.0779e-01,  4.0259e-01,  1.4621e+00,  2.2431e+00,  1.2530e-01,\n",
      "         4.4687e-01, -6.0598e-01,  6.6458e-02,  2.7199e+00,  6.0205e+00,\n",
      "        -2.2069e+00,  1.5209e+00, -9.1266e-01,  1.8191e+00,  1.5419e+00,\n",
      "        -2.4767e+00, -2.8653e+00,  2.1198e+00,  4.0584e+00, -8.4753e-01,\n",
      "        -8.5784e-01,  7.0232e-01, -8.6085e-01, -1.1790e+00, -3.1880e+00,\n",
      "        -2.2806e+00,  2.7435e+00,  2.2087e+00, -1.1238e+00, -6.8005e-01,\n",
      "         3.2202e+00,  3.6514e+00, -1.8388e+00, -4.7492e-01, -2.9633e-01,\n",
      "         1.0991e+00, -1.4126e+00,  3.8940e+00,  2.2684e+00,  2.6625e+00,\n",
      "        -1.4457e+00, -2.5330e+00, -5.0851e-02,  3.9207e+00, -2.0403e-01,\n",
      "         4.6893e+00, -1.3671e+00, -5.3490e-01, -3.2886e+00, -1.1634e+00,\n",
      "         1.3998e+00, -6.4206e-01,  1.0288e+00, -2.3622e+00, -7.9329e-01,\n",
      "         1.4189e+00,  2.8368e+00,  2.1367e+00,  2.9842e+00, -1.5496e+00,\n",
      "         7.9040e-01, -2.2696e+00, -1.8700e+00, -2.4107e+00, -2.3999e+00,\n",
      "         2.7465e-01, -1.8731e+00,  3.4889e+00, -2.2360e+00,  6.6773e-01,\n",
      "        -3.0871e+00, -2.9593e+00,  1.4262e+00,  1.3273e+00,  1.8952e+00,\n",
      "         3.2831e+00,  9.7214e-01,  4.3729e+00,  3.3936e-01,  1.5345e+00,\n",
      "        -5.6259e-01,  8.0429e-01,  5.0106e+00, -2.8867e+00, -5.3508e-01,\n",
      "         3.3999e+00, -2.4959e+00, -2.6279e-01,  2.3438e+00,  3.7554e+00,\n",
      "         2.3136e-01,  1.4029e+00, -5.5190e-01,  2.3644e+00, -9.7520e-01,\n",
      "         1.0800e+00, -2.1340e+00, -3.4179e-01,  2.8654e+00, -9.8916e-01,\n",
      "         3.5481e+00,  2.3864e+00,  8.0835e-01,  1.4495e+00,  2.6721e-01,\n",
      "         8.8966e-01, -1.1843e+00,  1.6590e+00, -2.8995e+00, -1.5644e+00,\n",
      "         1.6921e+00, -8.7423e-01, -1.7818e+00, -2.2707e+00,  8.1145e-01,\n",
      "        -3.8444e-01, -9.3532e-01, -2.7903e+00,  2.4821e-01, -4.6827e-01,\n",
      "         3.0601e+00, -5.4850e-01,  4.9441e+00,  1.9084e+00,  1.1258e+00,\n",
      "        -7.4230e-01,  3.4724e-02, -1.4819e+00,  5.2622e-01,  3.2890e+00,\n",
      "         3.0236e+00, -2.5787e+00,  3.1385e+00,  8.2233e-01,  1.4217e+00,\n",
      "         2.0526e+00,  1.8203e+00,  2.0763e-01, -2.6140e+00,  1.3327e+00,\n",
      "         4.4004e-01,  1.4338e+00, -3.1662e+00,  1.6662e-01, -5.3523e-01,\n",
      "         1.8504e+00, -4.1172e-01,  9.3392e+00,  3.9265e+00, -1.4186e+00,\n",
      "         8.0187e-01, -1.5843e+00,  4.6577e-01, -1.4532e+00,  1.8671e+00,\n",
      "         4.0508e-01,  7.4947e-01,  2.8021e+00,  2.0199e+00,  7.0932e-01,\n",
      "         2.4169e+00, -6.8565e-01, -3.0119e+00,  7.6053e-01, -5.2808e-01,\n",
      "         1.6696e+00,  4.3526e+00, -2.7737e+00, -1.9053e+00, -9.1880e-01,\n",
      "        -1.5849e-01, -7.4726e-01,  4.6400e-01,  5.7536e-01,  3.7364e+00,\n",
      "         3.3258e+00,  5.8588e+00,  2.7710e+00, -1.6095e+00, -3.9247e+00,\n",
      "         4.1651e+00, -8.2594e-02,  2.0041e+00, -5.1869e-01,  3.2742e+00,\n",
      "         2.7844e+00, -1.8448e+00, -3.0698e+00,  4.5109e+00,  9.1718e-01,\n",
      "         2.7700e+00,  2.1773e+00,  3.2675e+00, -6.1139e-01,  1.5754e+00,\n",
      "         4.4944e+00, -1.2613e-01,  2.3093e+00,  2.9302e+00, -2.2458e+00,\n",
      "        -4.1744e-01,  1.3690e-01,  2.0555e+00,  1.1911e+00, -3.7637e-01,\n",
      "        -4.6738e-01, -2.1169e+00, -9.0976e-01, -6.4427e-01, -4.0605e-01,\n",
      "         2.4955e+00,  2.6239e+00, -3.8044e-01, -6.3270e-01,  1.6349e-01,\n",
      "         3.0852e+00, -1.3513e+00, -8.8318e-03,  2.2713e+00, -1.7835e+00,\n",
      "        -1.3492e+00,  2.8153e+00,  4.7853e+00,  4.7259e+00,  6.1973e+00,\n",
      "         4.4492e+00, -5.5137e-01,  2.1875e+00,  1.5885e+00, -1.4409e+00,\n",
      "        -4.7305e+00, -1.1434e+00,  4.7514e+00, -1.1232e+00, -1.2922e+00,\n",
      "        -2.0055e+00,  2.9085e+00,  2.2002e+00,  6.0351e+00,  5.0340e-02,\n",
      "        -1.5007e+00,  6.0202e-01,  4.2572e-01,  3.2509e-01, -2.3255e-01,\n",
      "        -2.9469e+00,  1.5691e+00,  5.4889e+00,  2.9347e+00, -5.6415e-01,\n",
      "         1.2692e+00,  1.5996e+00, -4.6633e+00,  8.0551e+00,  1.1217e+00,\n",
      "         4.5744e-01, -2.5805e-01, -1.3869e-01, -6.8227e-01, -1.3691e+00,\n",
      "         5.5030e-01, -9.3848e-01, -8.0488e-01,  1.0915e+00,  1.9763e+00,\n",
      "        -1.6200e+00, -1.0742e+00, -5.8378e-01, -2.7406e+00,  4.3845e-01,\n",
      "         9.3701e-01,  6.6514e+00,  6.8016e-01,  2.0877e+00,  6.2903e-01,\n",
      "         1.1679e+00, -3.0709e+00,  3.9992e+00,  1.7318e+00, -1.6903e+00,\n",
      "         1.7701e-01, -7.2466e-01, -2.4024e-01,  7.6981e-02,  7.0561e-01,\n",
      "        -3.8639e-01,  3.0557e-01, -3.6913e+00, -1.6812e+00,  2.5184e+00,\n",
      "         2.3382e+00, -3.1607e-01,  7.8336e-01, -2.6275e+00,  4.9198e-02,\n",
      "         1.2864e+00, -2.1867e+00,  3.8510e-01, -2.5063e+00,  4.6966e+00,\n",
      "         1.8905e+00,  3.0804e+00,  3.5198e+00,  3.3505e+00,  1.2403e+00,\n",
      "         4.0947e+00, -1.8232e+00, -1.7203e+00,  4.3189e-01,  1.4282e+00,\n",
      "         1.5222e+00,  3.7362e+00,  1.7975e+00, -4.1750e-02, -8.1148e-01,\n",
      "         2.9349e+00,  5.3238e-01,  1.0454e+00, -6.0881e-01,  3.2184e+00,\n",
      "        -3.8990e-01, -2.5584e+00,  8.3683e-01,  7.0508e+00, -1.3443e+00,\n",
      "         2.1603e+00, -7.5407e-01,  3.9565e+00, -6.4300e-02, -3.1335e+00,\n",
      "        -2.1087e+00, -1.0052e+00,  2.2330e+00, -6.0318e-01,  4.1483e+00,\n",
      "         1.7920e+00, -3.4098e-01,  4.5665e+00,  5.7752e+00,  5.2197e-01,\n",
      "         2.6424e+00,  6.3660e+00, -7.1398e-01, -6.1536e-01,  2.7307e+00,\n",
      "         4.5209e-01,  3.6574e-01, -2.2743e+00, -2.4845e+00,  8.0627e+00,\n",
      "         3.6370e+00,  1.5858e+00,  6.3196e-02,  5.5016e+00,  1.6633e+00,\n",
      "         2.9449e+00,  1.8551e+00,  4.6298e-01, -1.0638e+00,  5.9036e+00,\n",
      "        -3.7994e-01,  3.2158e-01, -2.2877e-01, -4.2729e-01,  1.8988e+00,\n",
      "         2.5817e+00,  3.4522e-01,  1.0521e+00, -1.3388e-01, -1.1919e+00,\n",
      "        -2.2159e+00,  1.6810e+00,  2.8018e+00,  8.7243e-01,  2.3994e+00,\n",
      "        -1.2382e+00, -1.5118e+00, -1.6602e+00, -1.2616e+00, -9.1343e-01,\n",
      "        -1.2382e+00, -9.5586e-01,  2.5402e+00,  1.2362e+00,  5.2784e+00,\n",
      "        -1.8813e-01,  2.2721e+00,  8.3437e+00, -7.8273e-01,  2.5566e+00,\n",
      "         4.1420e-01,  1.3221e+00,  2.4079e+00,  2.4870e+00,  5.3819e+00,\n",
      "         1.3965e+00, -4.2355e-01,  7.9549e-01, -3.5940e+00,  4.4284e+00,\n",
      "         5.8142e+00,  1.3471e+00,  7.6537e-01, -6.4643e-01,  4.4451e+00,\n",
      "         1.3838e+00, -1.5654e+00, -2.6618e+00,  4.0099e+00, -2.1334e-01,\n",
      "        -2.6419e-01, -2.2898e+00,  7.9743e-01, -1.1392e+00, -2.6845e+00,\n",
      "         2.0379e+00,  5.0848e-01,  8.8810e-01,  7.5647e-01, -1.9035e+00,\n",
      "         4.4961e+00, -1.8661e+00, -1.3762e+00, -4.0792e-01,  2.3275e+00,\n",
      "         5.1901e-01,  5.2354e-01, -2.1084e+00,  2.7529e+00, -1.2638e+00,\n",
      "        -2.8136e+00, -7.6054e-01,  6.3122e+00,  3.9594e+00, -2.0113e+00,\n",
      "         5.0120e-01,  2.1308e+00,  1.2456e+00, -1.2476e+00,  5.5637e+00,\n",
      "         6.7278e+00, -4.9754e-01, -4.6037e-01, -1.7853e+00, -3.1000e+00,\n",
      "        -1.3840e+00, -4.5190e+00, -2.6011e+00, -8.8881e-01, -9.0140e-01,\n",
      "        -3.7117e-01, -2.5812e+00,  1.1925e+00, -4.6692e+00,  1.7009e-01,\n",
      "        -3.8100e+00,  1.4755e+00, -1.9202e+00, -8.9815e-01, -1.6283e+00,\n",
      "        -4.0565e-01, -1.4393e+00, -2.3743e+00, -1.3688e+00,  3.4582e-01,\n",
      "        -2.1270e+00,  1.4151e+00, -2.2726e+00,  1.3476e+00, -1.0415e-01,\n",
      "         6.6053e-02, -3.7820e-01, -1.6164e+00,  2.9187e-01, -1.3046e+00,\n",
      "        -1.0819e+00,  9.3751e-01, -2.5769e+00,  2.1913e-01, -3.0456e+00,\n",
      "        -3.4710e+00, -3.2549e+00, -3.1922e+00, -1.7685e+00, -1.3510e+00,\n",
      "        -4.8971e+00, -1.2438e+00, -3.4582e+00, -1.6431e+00, -2.2775e+00,\n",
      "         5.3887e+00,  3.7530e+00,  3.1779e+00, -7.0350e-01,  1.7453e+00,\n",
      "         6.8441e+00,  4.3374e+00,  3.8504e+00,  7.4715e+00,  4.1745e+00,\n",
      "         1.3748e+00, -1.4188e-01,  1.3852e+00, -1.0718e+00,  1.3357e+00,\n",
      "         2.2325e+00, -6.3353e-01, -2.0195e+00,  8.6318e-01, -2.2088e+00,\n",
      "        -7.6400e-01, -2.5418e+00, -2.4728e+00, -4.4958e+00, -1.6897e+00,\n",
      "         3.8868e-01,  1.0807e+00, -2.8856e+00, -7.1222e-01, -1.1738e+00],\n",
      "       device='cuda:0')\n",
      "tensor([5.9540e-06, 1.1178e-05, 3.3942e-06, 1.2949e-06, 6.5434e-06, 2.5078e-05,\n",
      "        7.7925e-07, 3.6087e-06, 3.5578e-06, 5.2018e-06, 6.0163e-06, 1.2712e-05,\n",
      "        9.6078e-07, 1.0902e-06, 5.1795e-07, 7.3680e-07, 1.8012e-06, 1.0734e-07,\n",
      "        3.8731e-06, 1.5042e-06, 8.3905e-07, 1.1461e-06, 5.6324e-06, 1.6982e-05,\n",
      "        1.2796e-06, 1.3431e-06, 1.8953e-05, 3.1138e-05, 2.0357e-05, 7.0972e-06,\n",
      "        5.3976e-06, 8.1522e-05, 1.4921e-05, 1.5054e-05, 3.4540e-05, 1.5292e-06,\n",
      "        7.1664e-06, 3.6042e-07, 4.4159e-05, 7.6731e-06, 8.5033e-06, 8.5794e-06,\n",
      "        6.3580e-07, 5.4176e-06, 8.6322e-06, 2.7084e-06, 2.4427e-06, 5.2459e-05,\n",
      "        2.4224e-06, 4.2113e-06, 2.1131e-06, 5.9434e-05, 2.5073e-05, 2.8933e-05,\n",
      "        8.2548e-07, 4.3116e-06, 1.9118e-06, 3.5228e-06, 2.6882e-05, 1.5774e-05,\n",
      "        7.4087e-04, 4.0357e-06, 6.2080e-06, 9.5473e-06, 8.9761e-06, 1.6865e-05,\n",
      "        2.1089e-05, 1.3956e-05, 8.5998e-06, 2.7471e-06, 2.5106e-05, 9.6969e-06,\n",
      "        3.6723e-07, 6.1596e-05, 1.6476e-06, 8.9348e-06, 1.1847e-05, 3.2294e-06,\n",
      "        7.9280e-05, 9.4389e-05, 5.5681e-06, 6.8302e-07, 1.8981e-06, 1.0814e-06,\n",
      "        8.6084e-06, 7.9111e-07, 7.3199e-06, 3.0002e-07, 7.2595e-07, 2.5756e-06,\n",
      "        7.2106e-07, 1.1366e-07, 1.1663e-05, 1.2491e-06, 4.0755e-06, 2.0142e-06,\n",
      "        6.3032e-06, 1.0759e-05, 3.5755e-06, 1.3630e-05, 1.6035e-05, 5.6734e-06,\n",
      "        8.5453e-06, 2.2123e-06, 6.9981e-06, 8.1522e-07, 8.2506e-07, 2.4869e-05,\n",
      "        1.4598e-04, 8.2338e-07, 1.1898e-05, 4.1813e-04, 1.2372e-05, 1.5108e-04,\n",
      "        3.2954e-05, 3.1669e-06, 2.5616e-05, 1.2240e-05, 1.4915e-05, 1.1549e-05,\n",
      "        1.2677e-04, 3.9385e-06, 4.3448e-07, 1.3699e-06, 2.1971e-06, 6.8335e-06,\n",
      "        4.0898e-04, 1.1441e-06, 1.8401e-05, 6.2498e-06, 1.0898e-05, 8.1062e-07,\n",
      "        4.9219e-06, 2.2473e-07, 2.9018e-06, 5.1732e-06, 2.6133e-06, 5.0134e-06,\n",
      "        2.4338e-07, 2.6346e-06, 6.9855e-06, 9.7588e-06, 2.3044e-06, 5.6016e-06,\n",
      "        5.6221e-05, 4.9315e-06, 7.5195e-06, 2.8790e-05, 2.3797e-05, 1.7776e-05,\n",
      "        1.0262e-05, 1.0151e-04, 5.4334e-05, 2.3070e-04, 7.3447e-05, 2.1351e-04,\n",
      "        4.6973e-06, 1.2499e-04, 9.0177e-05, 1.4325e-05, 3.1341e-05, 6.5086e-06,\n",
      "        3.2973e-06, 1.3276e-05, 5.8045e-05, 1.1581e-05, 2.0962e-05, 1.4941e-05,\n",
      "        1.0547e-04, 9.9794e-05, 1.1764e-05, 4.9054e-05, 2.3906e-05, 7.5822e-06,\n",
      "        3.3856e-06, 4.8101e-06, 1.6401e-05, 9.2123e-06, 4.8156e-06, 7.5518e-06,\n",
      "        3.1449e-06, 2.4134e-06, 5.6216e-06, 1.5774e-05, 3.8734e-06, 4.4671e-05,\n",
      "        3.5109e-05, 7.1341e-05, 7.0496e-05, 1.6143e-05, 4.9618e-06, 4.6248e-06,\n",
      "        3.3820e-05, 4.0771e-05, 1.8873e-06, 2.0539e-05, 5.4333e-05, 6.2247e-06,\n",
      "        1.0651e-06, 1.3243e-05, 1.3486e-04, 1.4324e-05, 1.1825e-05, 1.2260e-04,\n",
      "        7.1838e-05, 3.4173e-07, 2.4323e-06, 2.8598e-05, 6.9307e-05, 1.7165e-05,\n",
      "        1.8090e-05, 1.2436e-05, 6.0041e-06, 8.0895e-07, 1.2461e-06, 2.3743e-06,\n",
      "        2.2226e-06, 1.4346e-06, 2.3545e-06, 1.0619e-05, 3.5343e-06, 3.3082e-06,\n",
      "        3.9833e-06, 1.1222e-04, 4.0364e-05, 1.0517e-06, 9.1367e-06, 9.7409e-06,\n",
      "        1.3807e-05, 5.2950e-05, 2.8026e-04, 5.7347e-04, 3.1211e-05, 1.7375e-05,\n",
      "        1.4098e-05, 8.8910e-06, 2.5325e-05, 2.2900e-05, 2.7625e-06, 2.7281e-06,\n",
      "        5.2050e-06, 2.0172e-06, 4.3919e-06, 4.1505e-06, 7.0465e-06, 7.1396e-05,\n",
      "        6.0928e-06, 1.3173e-05, 6.4717e-05, 6.5100e-05, 1.0989e-04, 2.3510e-05,\n",
      "        2.6459e-05, 4.4335e-05, 3.1430e-05, 2.0911e-06, 2.3857e-05, 4.1424e-04,\n",
      "        1.2698e-04, 2.6990e-03, 3.6710e-05, 3.9540e-05, 1.2265e-05, 5.5309e-05,\n",
      "        2.4813e-05, 2.1523e-05, 1.6110e-05, 2.1955e-05, 1.3154e-05, 1.5746e-05,\n",
      "        5.1384e-05, 1.1331e-06, 1.5460e-04, 3.0465e-04, 2.1487e-05, 5.3007e-06,\n",
      "        3.5959e-06, 9.5295e-05, 1.9132e-05, 1.4364e-04, 1.6317e-05, 9.4388e-06,\n",
      "        1.5977e-06, 4.6866e-05, 4.3814e-06, 1.5172e-05, 2.7828e-06, 1.6902e-06,\n",
      "        5.5455e-06, 2.2109e-06, 1.2304e-06, 3.5358e-06, 1.0232e-06, 1.7219e-05,\n",
      "        3.5183e-06, 4.8375e-06, 4.3243e-06, 4.9483e-06, 1.7987e-06, 2.8896e-05,\n",
      "        5.6667e-06, 5.0003e-05, 7.9125e-06, 2.6422e-05, 1.3043e-05, 1.7626e-06,\n",
      "        2.4313e-06, 5.9437e-06, 7.7446e-06, 1.0450e-05, 2.3845e-03, 2.8610e-06,\n",
      "        2.0964e-05, 1.8809e-05, 2.8421e-06, 2.1503e-05, 2.6868e-06, 5.4640e-06,\n",
      "        7.8515e-05, 2.9187e-05, 1.7904e-05, 1.6856e-06, 5.8930e-06, 2.1419e-06,\n",
      "        1.0795e-05, 6.1310e-06, 5.0011e-06, 3.5426e-05, 3.5920e-05, 1.7482e-06,\n",
      "        2.7011e-05, 1.7074e-05, 2.4220e-05, 5.1459e-06, 2.2891e-05, 4.2832e-05,\n",
      "        2.1015e-06, 3.0760e-05, 3.4406e-06, 1.4064e-05, 6.1686e-06, 1.9445e-05,\n",
      "        4.5718e-06, 6.7071e-07, 2.4788e-06, 4.4249e-05, 1.0168e-05, 2.3754e-05,\n",
      "        8.0961e-06, 2.4323e-04, 3.1923e-05, 1.1419e-06, 6.3375e-06, 1.2121e-05,\n",
      "        2.6539e-05, 7.6093e-06, 4.5368e-06, 2.8981e-06, 4.6415e-06, 6.3205e-06,\n",
      "        1.2981e-06, 1.9606e-04, 1.2522e-05, 1.2014e-05, 2.9628e-06, 1.9830e-06,\n",
      "        3.3944e-07, 1.7117e-06, 5.5371e-06, 5.2915e-06, 2.9528e-06, 8.5668e-06,\n",
      "        4.1008e-05, 2.7764e-04, 4.3397e-05, 4.5389e-06, 1.5837e-05, 2.9165e-05,\n",
      "        2.2787e-05, 2.2698e-06, 5.0715e-04, 6.6536e-06, 5.7272e-05, 9.9027e-06,\n",
      "        5.5981e-06, 1.5208e-05, 1.7005e-06, 2.6990e-06, 5.4578e-07, 1.4664e-05,\n",
      "        8.1414e-06, 6.0553e-07, 2.0248e-06, 2.7706e-06, 6.3713e-06, 2.7472e-06,\n",
      "        5.7935e-07, 1.5688e-06, 3.6152e-06, 1.6599e-04, 9.2695e-07, 2.7381e-05,\n",
      "        4.9400e-06, 5.1383e-03, 1.8747e-03, 7.0421e-04, 3.8683e-06, 1.4733e-03,\n",
      "        4.9193e-04, 1.6563e-04, 2.7590e-05, 2.4206e-06, 5.3104e-04, 5.1799e-05,\n",
      "        7.8578e-06, 8.6789e-05, 7.6202e-04, 1.2018e-03, 4.2534e-04, 7.6022e-05,\n",
      "        4.9261e-06, 4.1912e-02, 2.2751e-05, 5.3139e-06, 5.4694e-06, 4.4470e-05,\n",
      "        1.0175e-05, 2.2035e-05, 5.6477e-04, 5.2637e-06, 7.7726e-05, 3.8123e-06,\n",
      "        2.6330e-05, 4.6792e-05, 1.2913e-06, 5.2281e-05, 1.7920e-03, 2.0958e-03,\n",
      "        6.7334e-06, 1.8027e-05, 1.2554e-05, 2.9016e-05, 1.9953e-04, 7.3055e-07,\n",
      "        4.8495e-04, 3.0090e-05, 1.9029e-03, 1.3417e-04, 1.2824e-05, 4.2649e-05,\n",
      "        3.2499e-05, 1.7568e-05, 4.4746e-06, 5.6737e-06, 2.3380e-06, 1.3664e-05,\n",
      "        2.1861e-04, 1.0775e-05, 1.7378e-05, 5.8624e-05, 2.6885e-02, 1.9482e-05,\n",
      "        8.7047e-05, 3.5219e-05, 2.2311e-05, 3.6110e-05, 8.4403e-03, 1.8938e-06,\n",
      "        6.8146e-02, 4.3512e-06, 7.9660e-06, 6.3713e-05, 2.5985e-05, 2.9105e-06,\n",
      "        4.1672e-05, 8.3458e-03, 1.0067e-03, 7.3479e-06, 1.0666e-05, 9.9936e-04,\n",
      "        2.6850e-05, 3.2625e-05, 9.4121e-05, 2.0553e-04, 2.4724e-05, 3.4102e-05,\n",
      "        1.1900e-05, 2.3311e-05, 3.3109e-04, 8.9816e-03, 2.4002e-06, 9.9822e-05,\n",
      "        8.7567e-06, 1.3450e-04, 1.0193e-04, 1.8327e-06, 1.2426e-06, 1.8168e-04,\n",
      "        1.2626e-03, 9.3461e-06, 9.2502e-06, 4.4027e-05, 9.2223e-06, 6.7094e-06,\n",
      "        8.9988e-07, 2.2297e-06, 3.3901e-04, 1.9858e-04, 7.0900e-06, 1.1050e-05,\n",
      "        5.4604e-04, 8.4037e-04, 3.4683e-06, 1.3566e-05, 1.6218e-05, 6.5466e-05,\n",
      "        5.3115e-06, 1.0711e-03, 2.1079e-04, 3.1263e-04, 5.1388e-06, 1.7324e-06,\n",
      "        2.0731e-05, 1.1002e-03, 1.7787e-05, 2.3727e-03, 5.5587e-06, 1.2776e-05,\n",
      "        8.1376e-07, 6.8148e-06, 8.8439e-05, 1.1478e-05, 6.1023e-05, 2.0551e-06,\n",
      "        9.8670e-06, 9.0138e-05, 3.7213e-04, 1.8478e-04, 4.3123e-04, 4.6314e-06,\n",
      "        4.8081e-05, 2.2543e-06, 3.3618e-06, 1.9578e-06, 1.9789e-06, 2.8707e-05,\n",
      "        3.3513e-06, 7.1435e-04, 2.3315e-06, 4.2530e-05, 9.9544e-07, 1.1311e-06,\n",
      "        9.0807e-05, 8.2248e-05, 1.4514e-04, 5.8148e-04, 5.7663e-05, 1.7292e-03,\n",
      "        3.0626e-05, 1.0119e-04, 1.2427e-05, 4.8753e-05, 3.2717e-03, 1.2163e-06,\n",
      "        1.2774e-05, 6.5349e-04, 1.7979e-06, 1.6772e-05, 2.2731e-04, 9.3254e-04,\n",
      "        2.7491e-05, 8.8714e-05, 1.2561e-05, 2.3203e-04, 8.2258e-06, 6.4232e-05,\n",
      "        2.5819e-06, 1.5498e-05, 3.8294e-04, 8.1118e-06, 7.5792e-04, 2.3721e-04,\n",
      "        4.8951e-05, 9.2940e-05, 2.8494e-05, 5.3098e-05, 6.6736e-06, 1.1461e-04,\n",
      "        1.2009e-06, 4.5632e-06, 1.1846e-04, 9.0998e-06, 3.6720e-06, 2.2519e-06,\n",
      "        4.9104e-05, 1.4851e-05, 8.5605e-06, 1.3393e-06, 2.7958e-05, 1.3656e-05,\n",
      "        4.6524e-04, 1.2604e-05, 3.0614e-03, 1.4707e-04, 6.7238e-05, 1.0383e-05,\n",
      "        2.2583e-05, 4.9557e-06, 3.6918e-05, 5.8492e-04, 4.4857e-04, 1.6549e-06,\n",
      "        5.0320e-04, 4.9641e-05, 9.0397e-05, 1.6989e-04, 1.3466e-04, 2.6846e-05,\n",
      "        1.5976e-06, 8.2700e-05, 3.3870e-05, 9.1499e-05, 9.1965e-07, 2.5767e-05,\n",
      "        1.2772e-05, 1.3878e-04, 1.4451e-05, 2.4813e-01, 1.1065e-03, 5.2797e-06,\n",
      "        4.8635e-05, 4.4737e-06, 3.4752e-05, 5.1000e-06, 1.4112e-04, 3.2706e-05,\n",
      "        4.6152e-05, 3.5947e-04, 1.6441e-04, 4.4336e-05, 2.4454e-04, 1.0988e-05,\n",
      "        1.0731e-06, 4.6666e-05, 1.2864e-05, 1.1583e-04, 1.6944e-03, 1.3618e-06,\n",
      "        3.2451e-06, 8.7031e-06, 1.8615e-05, 1.0332e-05, 3.4691e-05, 3.8778e-05,\n",
      "        9.1494e-04, 6.0685e-04, 7.6412e-03, 3.4846e-04, 4.3623e-06, 4.3075e-07,\n",
      "        1.4047e-03, 2.0083e-05, 1.6184e-04, 1.2985e-05, 5.7635e-04, 3.5313e-04,\n",
      "        3.4475e-06, 1.0127e-06, 1.9850e-03, 5.4579e-05, 3.4811e-04, 1.9243e-04,\n",
      "        5.7250e-04, 1.1835e-05, 1.0541e-04, 1.9524e-03, 1.9228e-05, 2.1959e-04,\n",
      "        4.0859e-04, 2.3087e-06, 1.4369e-05, 2.5013e-05, 1.7037e-04, 7.1775e-05,\n",
      "        1.4971e-05, 1.3669e-05, 2.6263e-06, 8.7822e-06, 1.1453e-05, 1.4533e-05,\n",
      "        2.6455e-04, 3.0079e-04, 1.4910e-05, 1.1586e-05, 2.5687e-05, 4.7710e-04,\n",
      "        5.6471e-06, 2.1621e-05, 2.1142e-04, 3.6654e-06, 5.6589e-06, 3.6422e-04,\n",
      "        2.6119e-03, 2.4610e-03, 1.0719e-02, 1.8662e-03, 1.2567e-05, 1.9441e-04,\n",
      "        1.0680e-04, 5.1632e-06, 1.9243e-07, 6.9521e-06, 2.5247e-03, 7.0943e-06,\n",
      "        5.9914e-06, 2.9358e-06, 3.9981e-04, 1.9690e-04, 9.1145e-03, 2.2939e-05,\n",
      "        4.8635e-06, 3.9825e-05, 3.3388e-05, 3.0192e-05, 1.7287e-05, 1.1452e-06,\n",
      "        1.0475e-04, 5.2784e-03, 4.1040e-04, 1.2408e-05, 7.7612e-05, 1.0799e-04,\n",
      "        2.0580e-07, 6.8709e-02, 6.6964e-05, 3.4464e-05, 1.6851e-05, 1.8988e-05,\n",
      "        1.1025e-05, 5.5476e-06, 3.7818e-05, 8.5335e-06, 9.7533e-06, 6.4973e-05,\n",
      "        1.5739e-04, 4.3168e-06, 7.4504e-06, 1.2167e-05, 1.4077e-06, 3.3816e-05,\n",
      "        5.5673e-05, 1.6880e-02, 4.3062e-05, 1.7595e-04, 4.0915e-05, 7.0131e-05,\n",
      "        1.0116e-06, 1.1900e-03, 1.2326e-04, 4.0235e-06, 2.6036e-05, 1.0568e-05,\n",
      "        1.7154e-05, 2.3558e-05, 4.4172e-05, 1.4822e-05, 2.9608e-05, 5.4398e-07,\n",
      "        4.0604e-06, 2.7066e-04, 2.2602e-04, 1.5901e-05, 4.7743e-05, 1.5761e-06,\n",
      "        2.2912e-05, 7.8956e-05, 2.4493e-06, 3.2059e-05, 1.7792e-06, 2.3900e-03,\n",
      "        1.4446e-04, 4.7478e-04, 7.3677e-04, 6.2204e-04, 7.5398e-05, 1.3092e-03,\n",
      "        3.5227e-06, 3.9048e-06, 3.3595e-05, 9.0986e-05, 9.9946e-05, 9.1480e-04,\n",
      "        1.3163e-04, 2.0921e-05, 9.6891e-06, 4.1051e-04, 3.7146e-05, 6.2046e-05,\n",
      "        1.1866e-05, 5.4504e-04, 1.4770e-05, 1.6889e-06, 5.0366e-05, 2.5168e-02,\n",
      "        5.6872e-06, 1.8920e-04, 1.0262e-05, 1.1402e-03, 2.0454e-05, 9.5028e-07,\n",
      "        2.6478e-06, 7.9826e-06, 2.0346e-04, 1.1933e-05, 1.3814e-03, 1.3091e-04,\n",
      "        1.5510e-05, 2.0986e-03, 7.0282e-03, 3.6762e-05, 3.0639e-04, 1.2689e-02,\n",
      "        1.0681e-05, 1.1789e-05, 3.3470e-04, 3.4280e-05, 3.1445e-05, 2.2438e-06,\n",
      "        1.8184e-06, 6.9231e-02, 8.2839e-04, 1.0651e-04, 2.3235e-05, 5.3461e-03,\n",
      "        1.1510e-04, 4.1463e-04, 1.3944e-04, 3.4656e-05, 7.5281e-06, 7.9915e-03,\n",
      "        1.4918e-05, 3.0086e-05, 1.7352e-05, 1.4228e-05, 1.4566e-04, 2.8834e-04,\n",
      "        3.0806e-05, 6.2462e-05, 1.9079e-05, 6.6234e-06, 2.3788e-06, 1.1715e-04,\n",
      "        3.5935e-04, 5.2191e-05, 2.4029e-04, 6.3236e-06, 4.8099e-06, 4.1467e-06,\n",
      "        6.1776e-06, 8.7500e-06, 6.3237e-06, 8.3865e-06, 2.7663e-04, 7.5087e-05,\n",
      "        4.2764e-03, 1.8072e-05, 2.1157e-04, 9.1690e-02, 9.9717e-06, 2.8119e-04,\n",
      "        3.3006e-05, 8.1822e-05, 2.4234e-04, 2.6229e-04, 4.7429e-03, 8.8148e-05,\n",
      "        1.4281e-05, 4.8326e-05, 5.9957e-07, 1.8279e-03, 7.3076e-03, 8.3897e-05,\n",
      "        4.6892e-05, 1.1428e-05, 1.8586e-03, 8.7032e-05, 4.5588e-06, 1.5230e-06,\n",
      "        1.2028e-03, 1.7622e-05, 1.6748e-05, 2.2093e-06, 4.8420e-05, 6.9819e-06,\n",
      "        1.4888e-06, 1.6740e-04, 3.6269e-05, 5.3015e-05, 4.6477e-05, 3.2509e-06,\n",
      "        1.9559e-03, 3.3751e-06, 5.5087e-06, 1.4506e-05, 2.2362e-04, 3.6653e-05,\n",
      "        3.6819e-05, 2.6487e-06, 3.4219e-04, 6.1634e-06, 1.3085e-06, 1.0195e-05,\n",
      "        1.2025e-02, 1.1435e-03, 2.9189e-06, 3.6006e-05, 1.8369e-04, 7.5801e-05,\n",
      "        6.2646e-06, 5.6884e-03, 1.8220e-02, 1.3262e-05, 1.3765e-05, 3.6591e-06,\n",
      "        9.8267e-07, 5.4659e-06, 2.3775e-07, 1.6183e-06, 8.9680e-06, 8.8558e-06,\n",
      "        1.5049e-05, 1.6508e-06, 7.1879e-05, 2.0459e-07, 2.5857e-05, 4.8311e-07,\n",
      "        9.5395e-05, 3.1971e-06, 8.8847e-06, 4.2809e-06, 1.4539e-05, 5.1715e-06,\n",
      "        2.0303e-06, 5.5491e-06, 3.0824e-05, 2.5998e-06, 8.9800e-05, 2.2476e-06,\n",
      "        8.3939e-05, 1.9655e-05, 2.3302e-05, 1.4944e-05, 4.3322e-06, 2.9205e-05,\n",
      "        5.9175e-06, 7.3932e-06, 5.5700e-05, 1.6579e-06, 2.7156e-05, 1.0376e-06,\n",
      "        6.7806e-07, 8.4166e-07, 8.9609e-07, 3.7211e-06, 5.6490e-06, 1.6290e-07,\n",
      "        6.2884e-06, 6.8677e-07, 4.2179e-06, 2.2367e-06, 4.7751e-03, 9.3025e-04,\n",
      "        5.2341e-04, 1.0794e-05, 1.2494e-04, 2.0468e-02, 1.6689e-03, 1.0255e-03,\n",
      "        3.8331e-02, 1.4179e-03, 8.6250e-05, 1.8927e-05, 8.7152e-05, 7.4685e-06,\n",
      "        8.2943e-05, 2.0335e-04, 1.1576e-05, 2.8949e-06, 5.1711e-05, 2.3957e-06,\n",
      "        1.0160e-05, 1.7171e-06, 1.8398e-06, 2.4333e-07, 4.0261e-06, 3.2174e-05,\n",
      "        6.4275e-05, 1.2176e-06, 1.0700e-05, 6.7444e-06], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(input_batch)\n",
    "# Tensor of shape 1000, with confidence scores over ImageNet's 1000 classes\n",
    "print(output[0])\n",
    "# The output has unnormalized scores. To get probabilities, you can run a softmax on it.\n",
    "probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "limousine 0.2481282651424408\n",
      "trailer truck 0.09169013053178787\n",
      "streetcar 0.0692308098077774\n",
      "pole 0.06870850175619125\n",
      "cab 0.06814604997634888\n"
     ]
    }
   ],
   "source": [
    "# Read the categories\n",
    "with open(\"docs/imagenet_classes.txt\", \"r\") as f:\n",
    "    categories = [s.strip() for s in f.readlines()]\n",
    "\n",
    "# Show top categories per image\n",
    "top5_prob, top5_catid = torch.topk(probabilities, 5)\n",
    "\n",
    "for i in range(top5_prob.size(0)):\n",
    "    print(categories[top5_catid[i]], top5_prob[i].item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
